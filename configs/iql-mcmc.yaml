# Configuration for Objective-Dimensional Soft Inverse Q-Learning (OD-SQIL) with MCMC SSM

# Expert data source
# - Environment name is auto-detected from train_dir path (dst_train -> deep_sea_treasure, highway_train -> mo-highway)
# - All environment settings are loaded from expert_dir/train_dir/config.yaml
expert_dir: "simulation_rl/dst_from_1_decay_1e-2/"
n_trajectories: 100  # Number of trajectories to use from expert dataset

# IQL training configuration
iql:
  hidden_dim: 256  # Network hidden dimension
  lr: 1.0e-6  # Learning rate for Q-network
  gamma: 0.99  # Discount factor
  tau: 0.005  # Soft update coefficient for target network
  mismatch_coef: 0.0  # Coefficient for mismatch regularization term (0 = disabled, SSM handles preference learning)
  n_updates: 10000  # Total number of training updates (one trajectory per update)

# State Space Model (SSM) for preference prediction
ssm_type: "ekf"  # Options: "pf" (particle filter), "kf" (kalman filter), "ekf" (extended kalman filter)

# Particle Filter settings (used if ssm_type == "pf")
particle_filter:
  n_particles: 1000
  process_noise: 0.01
  observation_noise: 0.05

# Kalman Filter settings (used if ssm_type == "kf")
kf:
  process_noise: 0.01
  observation_noise: 0.05
  initial_variance: 1.0e+3

# Extended Kalman Filter settings (used if ssm_type == "ekf")
ekf:
  process_noise: 0.05
  observation_noise: 0.1
  initial_variance: 1.0e+3
  beta: 1.0  # Temperature for margin->probability (higher = sharper distribution)

# MCMC settings for per-timestep noise parameter tuning
mcmc:
  interval: 1000  # Run MCMC every N training updates
  iterations_per_timestep: 20  # Number of MCMC iterations per timestep (keep low for speed)
  proposal_scale: 0.1  # Standard deviation of random walk proposals in log-space

# Device
device: "cuda"
seed: 42

# Logging
use_wandb: false
wandb_project: "MOIQL-MCMC"
save_dir: "moiql_mcmc_results"

# Evaluation
eval_interval: 1000  # Evaluate every N updates
early_stopping_patience: 0  # Stop training if no improvement for N evaluations (0 = disabled)
eval_weights: [0.5, 0.5]  # Weights [w1, w2] for computing eval_score = w1*cross_entropy + w2*preference_mae
