# Configuration for Objective-Dimensional Soft Inverse Q-Learning (OD-SQIL)

# Expert data source
# - Environment name is auto-detected from train_dir path (dst_train -> deep_sea_treasure, highway_train -> mo-highway)
# - All environment settings are loaded from expert_dir/train_dir/config.yaml
expert_dir: "simulation_rl/20251026_201017/"
n_trajectories: 100  # Number of trajectories to use from expert dataset

# Network architecture
hidden_dim: 256

# IQL-specific hyperparameters
lr: 1.0e-5  # Learning rate
gamma: 0.99  # Discount factor
mismatch_coef: 100.0  # Coefficient for mismatch regularization term (0 = disabled, SSM handles preference learning)
tau: 0.005  # Expectile parameter for value function update

# Training settings
n_updates: 10000  # Total number of training updates (one trajectory per update)

# State Space Model (SSM) for preference prediction
ssm_type: "mamba"  # Options: "pf" (particle filter), "ekf" (extended kalman filter), "mamba"

# Particle Filter settings (used if ssm_type == "pf")
particle_filter:
  n_particles: 1000
  process_noise: 0.05
  observation_noise: 0.05
  initial_noise: 2.0

# Extended Kalman Filter settings (used if ssm_type == "ekf")
ekf:
  process_noise: 0.01
  observation_noise: 0.1
  initial_noise: 0.1
  beta: 1.0  # Temperature for margin->probability (higher = sharper distribution)

# Mamba SSM settings (use if ssm_type == "mamba")
mamba:
  hidden_dim: 256
  learning_rate: 1.0e-5

# Device
device: "cuda"

# Logging
use_wandb: false
wandb_project: "MOIQL"
save_dir: "moiql_results"

# Evaluation
eval_interval: 100  # Evaluate every N updates
early_stopping_patience: 10  # Stop training if no improvement for N evaluations (0 = disabled)
eval_weights: [0.5, 0.5]  # Weights [w1, w2] for computing eval_score = w1*cross_entropy + w2*preference_mae
